{
  "subreddit": "webscraping",
  "post_id": "t3_1k8blav",
  "permalink": "https://www.reddit.com/r/webscraping/comments/1k8blav/scraping_coordinates_tried_everything_chatgpt/",
  "created_utc": "2025-04-26T12:16:05.367000+0000",
  "author": "godz_ares",
  "title": "Scraping coordinates, tried everything. ChatGPT even failed",
  "body": "Hi all, Context: I am creating a data engineering project. The aim is to create a tool where rock climbing crags (essentially a set of climbable rocks) are paired with weather data so someone could theoretically use this to plan which crags to climb in the next five days depending on the weather. There are no publicly available APIs and most websites such as UKC and theCrag have some sort of protection like Cloudflare. Because of this I am scraping a website called Crag27. Because this is my first scraping project I am scraping page by page, starting from the end point 'routes' and ending with the highest level 'continents'. After this, I want to adapt the code to create a fully working web crawler. The Problem: https://27crags.com/crags/brimham/topos/atlantis-31159 I want to scrape the coordinates of the crag.  This is important as I can use the coordinates as an argument when I use the weather API. That way I can pair the correct weather data with the correct crags. However, this is proving to be insanely difficulty. I started with Scrapy and used XPath notation: //div[@class=\"description\"]/text() and my code looked like this: import scrapy\nfrom scrapy.crawler import CrawlerProcess\nimport csv\nimport os\nimport pandas as pd\n\nclass CragScraper(scrapy.Spider):\n    name = 'crag_scraper'\n\n    def start_requests(self):\n        yield scrapy.Request(url='https://27crags.com/crags/brimham/topos/atlantis-31159', callback=self.parse)\n\n    def parse(self, response):\n        sector = response.xpath('//*[@id=\"sectors-dropdown\"]/span[1]/text()').get()\n        self.save_sector([sector])  # Changed to list to match save_routes method\n\n    def save_sector(self, sectors):  # Renamed to match the call in parse method\n        with open('sectors.csv', 'w', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerow(['sector'])\n            for sector in sectors:\n                writer.writerow([sector])\n\n# Create a CrawlerProcess instance to run the spider\nprocess = CrawlerProcess()\nprocess.crawl(CragScraper)\nprocess.start()\n\n# Read the saved routes from the CSV file\nsectors_df = pd.read_csv('sectors.csv')\nprint(sectors_df)  # Corrected variable name However, this didn't work. Being new and I out of ideas I asked ChatGPT what was wrong with the code and it bought me down a winding passage of using playwright, simulating a browser and intercepting an API call. Even after all the prompting in the world, ChatGPT gave up and recommended hard coding the coordinates. This all goes beyond my current understanding of scraping but I really want to do this project. This his how my code looks now: from playwright.sync_api import sync_playwright\nimport json\nimport csv\nimport pandas as pd\nfrom pathlib import Path\n\ndef scrape_sector_data():\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=False)  # Show browser\n        context = browser.new_context()\n        page = context.new_page()\n\n        # Intercept all network requests\n        sector_data = {}\n\n        def handle_response(response):\n            if 'graphql' in response.url:\n                try:\n                    json_response = response.json()\n                    if 'data' in json_response:\n                        # Look for 'topo' inside GraphQL data\n                        if 'topo' in json_response['data']:\n                            print(\"✅ Found topo data!\")\n                            sector_data.update(json_response['data']['topo'])\n                except Exception as e:\n                    pass  # Ignore non-JSON responses\n\n        page.on('response', handle_response)\n\n        # Go to the sector page\n        page.goto('https://27crags.com/crags/brimham/topos/atlantis-31159', wait_until=\"domcontentloaded\", timeout=60000)\n\n        # Give Playwright a few seconds to capture responses\n        page.wait_for_timeout(5000)\n\n        if sector_data:\n            # Save sector data\n            topo_name = sector_data.get('name', 'Unknown')\n            crag_name = sector_data.get('place', {}).get('name', 'Unknown')\n            lat = sector_data.get('place', {}).get('lat', 0)\n            lon = sector_data.get('place', {}).get('lon', 0)\n\n            print(f\"Topo Name: {topo_name}\")\n            print(f\"Crag Name: {crag_name}\")\n            print(f\"Latitude: {lat}\")\n            print(f\"Longitude: {lon}\")\n\n            with open('sectors.csv', 'w', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerow(['topo_name', 'crag_name', 'latitude', 'longitude'])\n                writer.writerow([topo_name, crag_name, lat, lon])\n\n        else:\n            print(\"❌ Could not capture sector data from network requests.\")\n\n        browser.close()\n\n# Run the scraper\nscrape_sector_data()\n\n# Read and display CSV if created\ncsv_path = Path('sectors.csv')\nif csv_path.exists():\n    sectors_df = pd.read_csv(csv_path)\n    print(\"\\nScraped Sector Data:\")\n    print(sectors_df)\nelse:\n    print(\"\\nCSV file was not created because no sector data was found.\") Can anyone lend me some help?",
  "all_comments": [
    {
      "user": "Proper-You-1262",
      "comment": "When will people learn? Unless you understand how to code, you're never going to build something like this by just copy and pasting code from AI. Vibe coding is super cringe and it always ends like this. The person asks really bad questions and posts their garbage code and ideas.",
      "replies": [
        {
          "user": "Practical-Hat-3943",
          "comment": "I hate getting old…. What is vibe coding? Is that what you call when you ask an LLM to give you the code and blindly copy/paste, or something else? If so, why is it call vibe coding?"
        },
        {
          "user": "Ok-Document6466",
          "comment": "I mean, 90% of these things *is* copy and paste, the other 10% is just finding the right selectors."
        },
        {
          "user": "godz_ares",
          "comment": "I understand the negative sentiment around vibe coding - but I really thought extracting coordinates would be far more simple than it is. I used ChatGPT as a last resort when my current understanding proved to be insufficient."
        }
      ]
    },
    {
      "user": "FeralFanatic",
      "comment": "import requests\nfrom bs4 import BeautifulSoup\n\ndef extract_lat_lon_from_description(html: str) -> tuple[float, float] | None:\n    soup = BeautifulSoup(html, \"html.parser\")\n    sector_properties_div = soup.find(\"div\", class_=\"sector-properties\")\n    description_div = sector_properties_div.find(\"div\", class_=\"description\")\n    if description_div:\n        text = description_div.get_text(strip=True)\n        coords = text.split(\",\")\n        if len(coords) == 2:\n            lat = float(coords[0].strip())\n            lon = float(coords[1].strip())\n            return lat, lon\n    return None\n\ndef main():\n    response = requests.get(\"https://27crags.com/crags/brimham/topos/atlantis-31159\")\n    coords = extract_lat_lon_from_description(response.text)\n    if coords:\n        lat, lon = coords\n        print(f\"{lat},{lon}\")\n\nif __name__ == \"__main__\":\n    main()",
      "replies": [
        {
          "user": "FeralFanatic",
          "comment": "If you use xpath and there's any slight change to the DOM tree then this will break. It may be the easiest but is not very robust. If you're going to use AI to help you, you need to formulate your questions better. Using the python library Scrapy create a parse method which can get the coords from the description div within the following html: <div class=\"sector-properties\" style=\"overflow-wrap: break-word;\">\n<a class=\"sector-property copytoclipboard\" data-href=\"54.079915, -1.685468\" data-msg-clicked=\"Coordinates has been copied to clipboard\" title=\"Copy coordinates to clipboard\" data-original-title=\"Coordinates has been copied to clipboard\">\n<i class=\"glyphicon glyphicon-map-marker\"></i>\n<div class=\"description\">54.079915, -1.685468</div>\n</a>\n</div> The response I got was the following: def parse(self, response, **kwargs):\n    coords = response.css('div.sector-properties a div.description::text').get()\n    if coords:\n        coords = coords.strip()\n        self.logger.info(f\"Extracted coordinates: {coords}\")\n        yield {\n            'coordinates': coords\n        }\n    else:\n        self.logger.warning(\"No coordinates found.\")\n    self.save_sector([sector])  # Changed to list to match save_routes method I tested this and it works."
        }
      ]
    },
    {
      "user": "zeeb0t",
      "comment": "Are the coordinates on the page without needing JavaScript rendering?",
      "replies": []
    },
    {
      "user": "egosaurusRex",
      "comment": "Selenium with undetected chromium driver should help you out.",
      "replies": []
    },
    {
      "user": "Ok-Document6466",
      "comment": "open that page in chrome and paste this into the javascript console (allow pasting): [...document.querySelectorAll('span.name,[data-href]')].map(el => el.innerText) If that's all you need just put it into playwright's page.evaluate() and you're done.",
      "replies": []
    },
    {
      "user": "Middle-Chard-4153",
      "comment": "selenium",
      "replies": []
    }
  ],
  "scraped_at": "2025-04-28T06:30:57+00:00Z"
}